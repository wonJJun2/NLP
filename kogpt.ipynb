{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  pad_token_id=tokenizer.eos_token_id,\n",
    "  torch_dtype='auto', low_cpu_mem_usage=True\n",
    ").to(device='cuda', non_blocking=True)\n",
    "_ = model.eval()\n",
    "\n",
    "prompt = '인간처럼 생각하고, 행동하는 \\'지능\\'을 통해 인류가 이제까지 풀지 못했던'\n",
    "\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.8, max_length=64)\n",
    "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "  \n",
    "print(generated)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "제목 : 그녀는 예뻤다\n",
    "내용 : 그녀는 내 옆으로 살포시 다가와 내 볼을 어루만지기 시작했다. \n",
    "그리고, 그녀는 말했다. \\'조금만 더 가까이 와줘..\\' 나는 그대로 다가갈 수 밖에 없었다.\n",
    "'''\n",
    "\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.85, max_length=512)\n",
    "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "  \n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "최고 핫한 인공지능, kogpt님과 인터뷰 나눠보겠습니다!\n",
    "Q : kogpt님, 수월한 대화가 가능하신가요?\n",
    "A : '''\n",
    "\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.85, max_length=512)\n",
    "  generated = tokenizer.batch_decode(gen_tokens)[0]\n",
    "  \n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '' #few-shot prompt \n",
    "\n",
    "\n",
    "prompt+= '' #few-shot prompt 내용\n",
    "\n",
    "for i in range():\n",
    "  prompt+=f\"{}\\n\\n\" # 기존 데이터 + 추가 학습 데이터\n",
    "\n",
    "\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

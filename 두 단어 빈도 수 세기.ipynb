{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "count = {} # 동시출현 빈도 dict\n",
    "for line in df:\n",
    "    words = re.sub(r\"[^a-z0-9]+\", \" \", line.lower()) # 소문자로 변환 .lower()\n",
    "    tokens = tokenizer.tokenize(words) # 토큰화 뒤 리스트 저장\n",
    "    stopped_tokens = [i for i in list(set(tokens)) if not i in stop_words+[\"br\"]] # stopwords 제거\n",
    "    stopped_tokens2 = [i for i in stopped_tokens if len(i)> 1] # 짧은 단어 제거\n",
    "    for i,a in enumerate(stopped_tokens2):\n",
    "        for b in stopped_tokens2[i+1:]:\n",
    "            if a>b:\n",
    "                count[b,a] = count.get((b,a),0) + 1\n",
    "            else:\n",
    "                count[a,b] = count.get((a,b),0) + 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
